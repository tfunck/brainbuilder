<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>brainbuilder.reconstruct API documentation</title>
<meta name="description" content="Main functions for launching 3D reconstruction with BrainBuilder." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>brainbuilder.reconstruct</code></h1>
</header>
<section id="section-intro">
<p>Main functions for launching 3D reconstruction with BrainBuilder.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Main functions for launching 3D reconstruction with BrainBuilder.&#34;&#34;&#34;

import argparse
import os

import pandas as pd

import brainbuilder.qc.quality_control as qc
from brainbuilder.downsample import downsample_sections
from brainbuilder.initalign import initalign
from brainbuilder.interpsections import interpolate_missing_sections
from brainbuilder.segment import segment
from brainbuilder.utils import utils
from brainbuilder.utils.validate_inputs import validate_inputs
from brainbuilder.volalign import multiresolution_alignment

base_file_dir, fn = os.path.split(os.path.abspath(__file__))
repo_dir = f&#34;{base_file_dir}/../&#34;

global file_dir
base_file_dir, fn = os.path.split(os.path.abspath(__file__))
file_dir = base_file_dir + os.sep + &#34;section_numbers&#34; + os.sep
manual_dir = base_file_dir + os.sep + &#34;manual_points&#34; + os.sep


def setup_args(args: argparse.Namespace) -&gt; argparse.Namespace:
    &#34;&#34;&#34;Setup the parameters and filenames that will be used in the reconstruction.

    :param args:   user input arguments
    :return args:   user input arguments with some additional parameters tacked on in this function
    &#34;&#34;&#34;
    ###
    ### Parameters
    ###

    if args.chunk_info_csv is None:
        args.chunk_info_csv = base_file_dir + &#34;/scale_factors.json&#34;

    args.manual_tfm_dir = base_file_dir + &#34;transforms/&#34;

    if args.sect_info_fn is None:
        args.sect_info_fn = args.out_dir + &#34;/autoradiograph_info_volume_order.csv&#34;

    args.qc_dir = f&#34;{args.out_dir}/6_quality_control/&#34;
    os.makedirs(args.crop_dir, exist_ok=True)

    args.resolution_list = [float(r) for r in args.resolution_list]

    return args


def reconstruct(
    hemi_info_csv: str,
    chunk_info_csv: str,
    sect_info_csv: str,
    resolution_list: list,
    output_dir: str,
    pytorch_model_dir: str = f&#34;{repo_dir}/nnUNet/Dataset501_Brain/nnUNetTrainer__nnUNetPlans__2d/&#34;,
    interp_type: str = &#34;surf&#34;,
    output_csv: str = &#34;&#34;,
    n_depths: int = 0,
    use_nnunet: bool = True,
    num_cores: int = 0,
    surface_smoothing: int = 0,
    batch_correction_resolution: float = 0,
    skip_interp: bool = False,
    clobber: bool = False,
) -&gt; None:
    &#34;&#34;&#34;Reconstruct 2D histological sections to 3D volume using a structural reference volume (e.g., T1w MRI from brain donor, stereotaxic template).

     Processing Steps
        1. Segment
        2. Init Alignment (Rigid 2D, per chunk)
        3.1 GM MRI to autoradiograph volume (Nonlinear 3D, per chunk)
        3.2 Autoradiograph to GM MRI (2D nonlinear, per chunk)
        4. Interpolate missing vertices on sphere, interpolate back to 3D volume
        5. Quality control

    :param hemi_info_csv: str, path to csv file that contains information about hemispheres to reconstruct
    :param sect_info_csv : pandas dataframe, contains information about sections
    :param chunk_info_csv : str, path to json file that contains information about chunks
    :param resolution_list : list, resolutions to use for reconstruction
    :param output_dir : str, output directory where results will be put
    :param pytorch_model_dir : str, optional, path of directory of pytorch model to use for reconstruction
    :param num_cores : int, optional, number of cores to use for reconstruction, default=0 (use all cores)
    :return sect_info : pandas dataframe, contains updated information about sections with new fields for files produced
    &#34;&#34;&#34;
    downsample_dir = f&#34;{output_dir}/0_downsample/&#34;
    seg_dir = f&#34;{output_dir}/1_seg/&#34;
    initalign_dir = f&#34;{output_dir}/2_init_align/&#34;
    multires_align_dir = f&#34;{output_dir}/3_multires_align/&#34;
    interp_dir = f&#34;{output_dir}/4_interp/&#34;
    qc_dir = f&#34;{output_dir}/qc/&#34;

    valid_inputs_npz = f&#34;{output_dir}/valid_inputs&#34;

    num_cores = int(utils.set_cores(num_cores) / 2)

    maximum_resolution = resolution_list[-1]

    valid_inputs = validate_inputs(
        hemi_info_csv, chunk_info_csv, sect_info_csv, valid_inputs_npz
    )
    assert valid_inputs, &#34;Error: invalid inputs&#34;

    sect_info_csv = downsample_sections(
        chunk_info_csv,
        sect_info_csv,
        min(resolution_list),
        downsample_dir,
        clobber=clobber,
    )
    print(sect_info_csv)
    qc.data_set_quality_control(sect_info_csv, qc_dir, column=&#34;img&#34;)

    # Stage: Segment
    seg_df_csv = segment(
        chunk_info_csv,
        sect_info_csv,
        seg_dir,
        maximum_resolution,
        use_nnunet=use_nnunet,
        clobber=clobber,
    )

    qc.data_set_quality_control(seg_df_csv, qc_dir, column=&#34;seg&#34;)

    # Stage: Initial rigid aligment of sections
    init_sect_csv, init_chunk_csv = initalign(
        seg_df_csv, chunk_info_csv, initalign_dir, resolution_list, clobber=clobber
    )

    # Stage: Multiresolution alignment of sections to structural reference volume
    align_chunk_info_csv, align_sect_info_csv = multiresolution_alignment(
        hemi_info_csv,
        init_chunk_csv,
        init_sect_csv,
        resolution_list,
        multires_align_dir,
        num_cores=num_cores,
        clobber=clobber,
    )

    qc.data_set_quality_control(align_sect_info_csv, qc_dir, column=&#34;init_img&#34;)

    # Stage: Interpolate missing sections
    if not skip_interp:
        interpolate_missing_sections(
            hemi_info_csv,
            align_chunk_info_csv,
            align_sect_info_csv,
            maximum_resolution,
            interp_dir,
            n_depths=n_depths,
            surface_smoothing=surface_smoothing,
            batch_correction_resolution=batch_correction_resolution,
            clobber=clobber,
        )

    return output_csv


def setup_argparse() -&gt; argparse.ArgumentParser:
    &#34;&#34;&#34;Get user input arguments.

    :return parser: argparse.ArgumentParser
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(description=&#34;Process some integers.&#34;)

    parser.add_argument(
        dest=&#34;hemi_info_csv&#34;,
        type=str,
        help=&#34;Path to csv file containing hemisphere information. Mandatory columns: [sub, hemisphere, struct_ref_vol, gm_surf, wm_surf]&#34;,
    )
    parser.add_argument(
        dest=&#34;chunk_info_csv&#34;,
        type=str,
        help=&#34;Path to csv file containing chunk informatio. Mandatory columns: [ sub, hemisphere, chunk, direction, pixel_size_0, pixel_size_1, section_thickness]&#34;,
    )
    parser.add_argument(
        dest=&#34;sect_info_csv&#34;,
        type=str,
        help=&#34;Path to csv file containing section information. Mandatory columns: [sub, hemisphere, chunk, acquisition, raw, sample] &#34;,
    )

    parser.add_argument(
        dest=&#34;ref_vol_fn&#34;,
        type=str,
        help=&#34;Structural reference volume filename&#34;,
    )
    parser.add_argument(dest=&#34;output_dir&#34;, type=str, help=&#34;Output directory&#34;)

    parser.add_argument(
        &#34;--resolutions&#34;,
        &#34;-r&#34;,
        dest=&#34;resolution_list&#34;,
        nargs=&#34;+&#34;,
        default=[4.0, 3.0, 2.0, 1.0, 0.5, 0.25],
        type=float,
        help=&#34;Resolution list.&#34;,
    )
    parser.add_argument(
        &#34;--num-cores&#34;,
        &#34;-r&#34;,
        dest=&#34;num_cores&#34;,
        type=int,
        default=0,
        help=&#34;number of cores to use for multiprocessing&#34;,
    )
    parser.add_argument(
        &#34;--no-nnunet&#34;,
        dest=&#34;use_nnunet&#34;,
        default=True,
        action=&#34;store_false&#34;,
        help=&#34;Do not use nnUNet for segmentation&#34;,
    )
    parser.add_argument(
        &#34;--ndepths&#34;,
        dest=&#34;n_depths&#34;,
        type=int,
        default=0,
        help=&#34;number of mid-surface depths between gm and wm surface&#34;,
    )
    parser.add_argument(
        &#34;--pytorch-model&#34;,
        &#34;-m&#34;,
        dest=&#34;pytorch_model_dir&#34;,
        default=f&#34;{repo_dir}/nnUNet/Dataset501_Brain/nnUNetTrainer__nnUNetPlans__2d/&#34;,
        help=&#34;Numer of cores to use for segmentation (Default=0; This is will set the number of cores to use to the maximum number of cores availale)&#34;,
    )

    parser.add_argument(
        &#34;--batch-correction&#34;,
        dest=&#34;batch_correction_resolution&#34;,
        default=0,
        help=&#34; Resolution at which to correct batch effects (Default=0, no correction)&#34;,
    )

    parser.add_argument(
        &#34;surface_smoothing&#34;,
        dest=&#34;surface_smoothing&#34;,
        default=0,
        help=&#34;Use surface smoothing beore creating final volume&#34;,
    )
    parser.add_argument(
        &#34;--clobber&#34;,
        dest=&#34;clobber&#34;,
        default=False,
        action=&#34;store_true&#34;,
        help=&#34;Overwrite existing results&#34;,
    )
    parser.add_argument(
        &#34;--skip-interp&#34;,
        dest=&#34;skip_interp&#34;,
        default=False,
        action=&#34;store_true&#34;,
        help=&#34;Skip interpolation step&#34;,
    )
    parser.add_argument(
        &#34;--debug&#34;, dest=&#34;debug&#34;, default=False, action=&#34;store_true&#34;, help=&#34;DEBUG mode&#34;
    )

    return parser


if __name__ == &#34;__main__&#34;:
    args = setup_argparse().parse_args()

    sect_info = pd.read_csv(args.sect_info_fn)

    reconstruct(
        args.hemi_info_csv,
        args.chunk_info_csv,
        args.sect_info_csv,
        args.ref_vol_fn,
        resolution_list=args.resolution_list,
        output_dir=args.output_dir,
        pytorch_model_dir=args.pytorch_model_dir,
        n_depths=args.n_depths,
        use_nnunet=args.use_nnunet,
        batch_correction=args.batch_correction_resolution,
        num_cores=args.num_cores,
        skip_interp=args.skip_interp,
    )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="brainbuilder.reconstruct.reconstruct"><code class="name flex">
<span>def <span class="ident">reconstruct</span></span>(<span>hemi_info_csv: str, chunk_info_csv: str, sect_info_csv: str, resolution_list: list, output_dir: str, pytorch_model_dir: str = '/home/tfunck/projects/BrainBuilder/brainbuilder/..//nnUNet/Dataset501_Brain/nnUNetTrainer__nnUNetPlans__2d/', interp_type: str = 'surf', output_csv: str = '', n_depths: int = 0, use_nnunet: bool = True, num_cores: int = 0, surface_smoothing: int = 0, batch_correction_resolution: float = 0, skip_interp: bool = False, clobber: bool = False) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Reconstruct 2D histological sections to 3D volume using a structural reference volume (e.g., T1w MRI from brain donor, stereotaxic template).</p>
<p>Processing Steps
1. Segment
2. Init Alignment (Rigid 2D, per chunk)
3.1 GM MRI to autoradiograph volume (Nonlinear 3D, per chunk)
3.2 Autoradiograph to GM MRI (2D nonlinear, per chunk)
4. Interpolate missing vertices on sphere, interpolate back to 3D volume
5. Quality control</p>
<p>:param hemi_info_csv: str, path to csv file that contains information about hemispheres to reconstruct
:param sect_info_csv : pandas dataframe, contains information about sections
:param chunk_info_csv : str, path to json file that contains information about chunks
:param resolution_list : list, resolutions to use for reconstruction
:param output_dir : str, output directory where results will be put
:param pytorch_model_dir : str, optional, path of directory of pytorch model to use for reconstruction
:param num_cores : int, optional, number of cores to use for reconstruction, default=0 (use all cores)
:return sect_info : pandas dataframe, contains updated information about sections with new fields for files produced</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reconstruct(
    hemi_info_csv: str,
    chunk_info_csv: str,
    sect_info_csv: str,
    resolution_list: list,
    output_dir: str,
    pytorch_model_dir: str = f&#34;{repo_dir}/nnUNet/Dataset501_Brain/nnUNetTrainer__nnUNetPlans__2d/&#34;,
    interp_type: str = &#34;surf&#34;,
    output_csv: str = &#34;&#34;,
    n_depths: int = 0,
    use_nnunet: bool = True,
    num_cores: int = 0,
    surface_smoothing: int = 0,
    batch_correction_resolution: float = 0,
    skip_interp: bool = False,
    clobber: bool = False,
) -&gt; None:
    &#34;&#34;&#34;Reconstruct 2D histological sections to 3D volume using a structural reference volume (e.g., T1w MRI from brain donor, stereotaxic template).

     Processing Steps
        1. Segment
        2. Init Alignment (Rigid 2D, per chunk)
        3.1 GM MRI to autoradiograph volume (Nonlinear 3D, per chunk)
        3.2 Autoradiograph to GM MRI (2D nonlinear, per chunk)
        4. Interpolate missing vertices on sphere, interpolate back to 3D volume
        5. Quality control

    :param hemi_info_csv: str, path to csv file that contains information about hemispheres to reconstruct
    :param sect_info_csv : pandas dataframe, contains information about sections
    :param chunk_info_csv : str, path to json file that contains information about chunks
    :param resolution_list : list, resolutions to use for reconstruction
    :param output_dir : str, output directory where results will be put
    :param pytorch_model_dir : str, optional, path of directory of pytorch model to use for reconstruction
    :param num_cores : int, optional, number of cores to use for reconstruction, default=0 (use all cores)
    :return sect_info : pandas dataframe, contains updated information about sections with new fields for files produced
    &#34;&#34;&#34;
    downsample_dir = f&#34;{output_dir}/0_downsample/&#34;
    seg_dir = f&#34;{output_dir}/1_seg/&#34;
    initalign_dir = f&#34;{output_dir}/2_init_align/&#34;
    multires_align_dir = f&#34;{output_dir}/3_multires_align/&#34;
    interp_dir = f&#34;{output_dir}/4_interp/&#34;
    qc_dir = f&#34;{output_dir}/qc/&#34;

    valid_inputs_npz = f&#34;{output_dir}/valid_inputs&#34;

    num_cores = int(utils.set_cores(num_cores) / 2)

    maximum_resolution = resolution_list[-1]

    valid_inputs = validate_inputs(
        hemi_info_csv, chunk_info_csv, sect_info_csv, valid_inputs_npz
    )
    assert valid_inputs, &#34;Error: invalid inputs&#34;

    sect_info_csv = downsample_sections(
        chunk_info_csv,
        sect_info_csv,
        min(resolution_list),
        downsample_dir,
        clobber=clobber,
    )
    print(sect_info_csv)
    qc.data_set_quality_control(sect_info_csv, qc_dir, column=&#34;img&#34;)

    # Stage: Segment
    seg_df_csv = segment(
        chunk_info_csv,
        sect_info_csv,
        seg_dir,
        maximum_resolution,
        use_nnunet=use_nnunet,
        clobber=clobber,
    )

    qc.data_set_quality_control(seg_df_csv, qc_dir, column=&#34;seg&#34;)

    # Stage: Initial rigid aligment of sections
    init_sect_csv, init_chunk_csv = initalign(
        seg_df_csv, chunk_info_csv, initalign_dir, resolution_list, clobber=clobber
    )

    # Stage: Multiresolution alignment of sections to structural reference volume
    align_chunk_info_csv, align_sect_info_csv = multiresolution_alignment(
        hemi_info_csv,
        init_chunk_csv,
        init_sect_csv,
        resolution_list,
        multires_align_dir,
        num_cores=num_cores,
        clobber=clobber,
    )

    qc.data_set_quality_control(align_sect_info_csv, qc_dir, column=&#34;init_img&#34;)

    # Stage: Interpolate missing sections
    if not skip_interp:
        interpolate_missing_sections(
            hemi_info_csv,
            align_chunk_info_csv,
            align_sect_info_csv,
            maximum_resolution,
            interp_dir,
            n_depths=n_depths,
            surface_smoothing=surface_smoothing,
            batch_correction_resolution=batch_correction_resolution,
            clobber=clobber,
        )

    return output_csv</code></pre>
</details>
</dd>
<dt id="brainbuilder.reconstruct.setup_argparse"><code class="name flex">
<span>def <span class="ident">setup_argparse</span></span>(<span>) ‑> argparse.ArgumentParser</span>
</code></dt>
<dd>
<div class="desc"><p>Get user input arguments.</p>
<p>:return parser: argparse.ArgumentParser</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_argparse() -&gt; argparse.ArgumentParser:
    &#34;&#34;&#34;Get user input arguments.

    :return parser: argparse.ArgumentParser
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(description=&#34;Process some integers.&#34;)

    parser.add_argument(
        dest=&#34;hemi_info_csv&#34;,
        type=str,
        help=&#34;Path to csv file containing hemisphere information. Mandatory columns: [sub, hemisphere, struct_ref_vol, gm_surf, wm_surf]&#34;,
    )
    parser.add_argument(
        dest=&#34;chunk_info_csv&#34;,
        type=str,
        help=&#34;Path to csv file containing chunk informatio. Mandatory columns: [ sub, hemisphere, chunk, direction, pixel_size_0, pixel_size_1, section_thickness]&#34;,
    )
    parser.add_argument(
        dest=&#34;sect_info_csv&#34;,
        type=str,
        help=&#34;Path to csv file containing section information. Mandatory columns: [sub, hemisphere, chunk, acquisition, raw, sample] &#34;,
    )

    parser.add_argument(
        dest=&#34;ref_vol_fn&#34;,
        type=str,
        help=&#34;Structural reference volume filename&#34;,
    )
    parser.add_argument(dest=&#34;output_dir&#34;, type=str, help=&#34;Output directory&#34;)

    parser.add_argument(
        &#34;--resolutions&#34;,
        &#34;-r&#34;,
        dest=&#34;resolution_list&#34;,
        nargs=&#34;+&#34;,
        default=[4.0, 3.0, 2.0, 1.0, 0.5, 0.25],
        type=float,
        help=&#34;Resolution list.&#34;,
    )
    parser.add_argument(
        &#34;--num-cores&#34;,
        &#34;-r&#34;,
        dest=&#34;num_cores&#34;,
        type=int,
        default=0,
        help=&#34;number of cores to use for multiprocessing&#34;,
    )
    parser.add_argument(
        &#34;--no-nnunet&#34;,
        dest=&#34;use_nnunet&#34;,
        default=True,
        action=&#34;store_false&#34;,
        help=&#34;Do not use nnUNet for segmentation&#34;,
    )
    parser.add_argument(
        &#34;--ndepths&#34;,
        dest=&#34;n_depths&#34;,
        type=int,
        default=0,
        help=&#34;number of mid-surface depths between gm and wm surface&#34;,
    )
    parser.add_argument(
        &#34;--pytorch-model&#34;,
        &#34;-m&#34;,
        dest=&#34;pytorch_model_dir&#34;,
        default=f&#34;{repo_dir}/nnUNet/Dataset501_Brain/nnUNetTrainer__nnUNetPlans__2d/&#34;,
        help=&#34;Numer of cores to use for segmentation (Default=0; This is will set the number of cores to use to the maximum number of cores availale)&#34;,
    )

    parser.add_argument(
        &#34;--batch-correction&#34;,
        dest=&#34;batch_correction_resolution&#34;,
        default=0,
        help=&#34; Resolution at which to correct batch effects (Default=0, no correction)&#34;,
    )

    parser.add_argument(
        &#34;surface_smoothing&#34;,
        dest=&#34;surface_smoothing&#34;,
        default=0,
        help=&#34;Use surface smoothing beore creating final volume&#34;,
    )
    parser.add_argument(
        &#34;--clobber&#34;,
        dest=&#34;clobber&#34;,
        default=False,
        action=&#34;store_true&#34;,
        help=&#34;Overwrite existing results&#34;,
    )
    parser.add_argument(
        &#34;--skip-interp&#34;,
        dest=&#34;skip_interp&#34;,
        default=False,
        action=&#34;store_true&#34;,
        help=&#34;Skip interpolation step&#34;,
    )
    parser.add_argument(
        &#34;--debug&#34;, dest=&#34;debug&#34;, default=False, action=&#34;store_true&#34;, help=&#34;DEBUG mode&#34;
    )

    return parser</code></pre>
</details>
</dd>
<dt id="brainbuilder.reconstruct.setup_args"><code class="name flex">
<span>def <span class="ident">setup_args</span></span>(<span>args: argparse.Namespace) ‑> argparse.Namespace</span>
</code></dt>
<dd>
<div class="desc"><p>Setup the parameters and filenames that will be used in the reconstruction.</p>
<p>:param args:
user input arguments
:return args:
user input arguments with some additional parameters tacked on in this function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_args(args: argparse.Namespace) -&gt; argparse.Namespace:
    &#34;&#34;&#34;Setup the parameters and filenames that will be used in the reconstruction.

    :param args:   user input arguments
    :return args:   user input arguments with some additional parameters tacked on in this function
    &#34;&#34;&#34;
    ###
    ### Parameters
    ###

    if args.chunk_info_csv is None:
        args.chunk_info_csv = base_file_dir + &#34;/scale_factors.json&#34;

    args.manual_tfm_dir = base_file_dir + &#34;transforms/&#34;

    if args.sect_info_fn is None:
        args.sect_info_fn = args.out_dir + &#34;/autoradiograph_info_volume_order.csv&#34;

    args.qc_dir = f&#34;{args.out_dir}/6_quality_control/&#34;
    os.makedirs(args.crop_dir, exist_ok=True)

    args.resolution_list = [float(r) for r in args.resolution_list]

    return args</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="brainbuilder" href="index.html">brainbuilder</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="brainbuilder.reconstruct.reconstruct" href="#brainbuilder.reconstruct.reconstruct">reconstruct</a></code></li>
<li><code><a title="brainbuilder.reconstruct.setup_argparse" href="#brainbuilder.reconstruct.setup_argparse">setup_argparse</a></code></li>
<li><code><a title="brainbuilder.reconstruct.setup_args" href="#brainbuilder.reconstruct.setup_args">setup_args</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>